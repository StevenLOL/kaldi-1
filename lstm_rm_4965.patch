Index: egs/rm/s5/local/nnet/run_lstm.sh
===================================================================
--- egs/rm/s5/local/nnet/run_lstm.sh	(revision 4965)
+++ egs/rm/s5/local/nnet/run_lstm.sh	(working copy)
@@ -45,16 +45,16 @@
   $cuda_cmd $dir/log/train_nnet.log \
     steps/nnet/train.sh --network-type lstm --learn-rate 0.0001 \
       --cmvn-opts "--norm-means=true --norm-vars=true" --feat-type plain --splice 0 \
-      --train-opts "--momentum 0.7 --halving-factor 0.8" \
+      --train-opts "--momentum 0.9 --halving-factor 0.5" \
       --train-tool "nnet-train-lstm-streams --num-stream=4 --targets-delay=5" \
+      --proto-opts "--num-cells 512 --num-recurrent 200 --num-layers 2 --clip-gradient 50.0" \
     ${train}_tr90 ${train}_cv10 data/lang $ali $ali $dir || exit 1;
 
   # Decode (reuse HCLG graph)
   steps/nnet/decode.sh --nj 20 --cmd "$decode_cmd" --config conf/decode_dnn.config --acwt 0.1 \
     $gmm/graph $dev $dir/decode || exit 1;
-  steps/nnet/decode.sh --nj 20 --cmd "$decode_cmd" --config conf/decode_dnn.config --acwt 0.1 \
-    --nnet-forward-opts "--no-softmax=true --prior-scale=1.0 --time-shift=5" \
-    $gmm/graph $dev $dir/decode_time-shift5 || exit 1;
+  steps/nnet/decode.sh --nj 20 --cmd "$decode_cmd" --config conf/decode_dnn.config --acwt $acwt \
+    $gmm/graph_ug $dev $dir/decode_ug || exit 1;
 fi
 
 # TODO : sequence training,
Index: egs/wsj/s5/steps/nnet/train.sh
===================================================================
--- egs/wsj/s5/steps/nnet/train.sh	(revision 4965)
+++ egs/wsj/s5/steps/nnet/train.sh	(working copy)
@@ -188,7 +188,7 @@
 fi
 
 # read the features,
-feats_tr="ark:copy-feats scp:$dir/train.scp ark:- |"
+feats_tr="ark:shuf $dir/train.scp | copy-feats scp:- ark:- |"
 feats_cv="ark:copy-feats scp:$dir/cv.scp ark:- |"
 # optionally add per-speaker CMVN,
 if [ ! -z "$cmvn_opts" ]; then
Index: egs/wsj/s5/utils/nnet/make_lstm_proto.py
===================================================================
--- egs/wsj/s5/utils/nnet/make_lstm_proto.py	(revision 4965)
+++ egs/wsj/s5/utils/nnet/make_lstm_proto.py	(working copy)
@@ -30,6 +30,8 @@
                    help='Number of LSTM cells [default: %default]');
 parser.add_option('--num-recurrent', dest='num_recurrent', type='int', default=512, 
                    help='Number of LSTM recurrent units [default: %default]');
+parser.add_option('--num-layers', dest='num_layers', type='int', default=2, 
+                   help='Number of LSTM layers [default: %default]');
 parser.add_option('--lstm-stddev-factor', dest='lstm_stddev_factor', type='float', default=0.01, 
                    help='Standard deviation of initialization [default: %default]');
 parser.add_option('--param-stddev-factor', dest='param_stddev_factor', type='float', default=0.04, 
@@ -53,8 +55,18 @@
 #</NnetProto>
 
 print "<NnetProto>"
-print "<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f" % \
-    (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)
+# normally we won't use more than 2 layers of LSTM
+if o.num_layers == 1:
+    print "<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f" % \
+        (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)
+elif o.num_layers == 2:
+    print "<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f" % \
+        (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)
+    print "<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f" % \
+        (o.num_recurrent, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)
+else:
+    sys.stderr.write("make_lstm_proto.py ERROR: more than 2 layers of LSTM, not supported yet.\n")
+    sys.exit(1)
 print "<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> 0.0 <BiasRange> 0.0 <ParamStddev> %f" % \
     (o.num_recurrent, num_leaves, o.param_stddev_factor)
 print "<Softmax> <InputDim> %d <OutputDim> %d" % \
